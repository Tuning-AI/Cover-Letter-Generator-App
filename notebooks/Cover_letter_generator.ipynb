{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uzt6KiJt4Uw"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 sentence_transformers langchain tiktoken duckduckgo-search xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login --token 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"
      ],
      "metadata": {
        "id": "b1ruDFUNuAn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM , BitsAndBytesConfig\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "DUbuguMVuFCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
        "    bnb_4bit_use_double_quant=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-2-7b-hf\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": 0})\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "model = PeftModel.from_pretrained(model, \"TuningAI/Llama2_7B_Cover_letter_generator\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\" , trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer,max_length=2048,temperature=0.5,top_p=0.95,repetition_penalty=1.15)\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "O6C1JhG4uWuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "[INST] <<SYS>>\n",
        "Given a user's information about the target job, you will generate a Cover letter for this job based on this information.\n",
        "<</SYS>>\n",
        "{text}[/INST]\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "U7dpsNaovYXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=local_llm ,prompt=prompt )"
      ],
      "metadata": {
        "id": "h1LMSzqtugBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"Enter your target job information and also your experience\"))"
      ],
      "metadata": {
        "id": "T-1d_Lqrvygd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}